# Setup: 

The model is trained on Wikipedia text for 100000 steps using a character level naive tokenizer.

# Result: 	

, Clooney Moltke's iconic storyline, unedited, based on the book-Birth Project of Sri Lanka and is acknowledged by Batman: Gould." "Brian" was "Therefore, that "It's value doesn't it's enough." In this movie, the story, Sports had two eyes in A two Malt final, Bobby Farrell left "Johnny Super Bulling" in 2013." No as saying, "We will give up to have the confidence of all both villains." Audiences polled by taking a director.

"Gched spring thaw took failed. In a press and an additional month later, "Up it have been the experience on that especially all the NBC: 'man's approaching intactness of the detective cartoon" list. By this time, Capp raised her that "was crazy, with the winning version at the Eriennial in 2020, but its completion in looking it, Motherwell said, "crofeunting, misidentified with Toby", their actions that has benefited in color and grittier like: this terms were to watch days calm and singing their sexual consequences of a sane". In early April 2016, Janet Stevens as Young "Don't 'Coffee broccoli' as her".

# Observation: 

Clearly improved language understanding is observed at a larger training steps, however the positional placement of tokens is still not grammatically suited for use as a langauge model. Transformers inherently are non-serial Deep Learning architectures, in order to pass in a component of serialization positional encoding is provided. We currently are using a simple nn.Embedd layer to pass the same, recent transoformers operate on "Sinusoidal Positional Encodings". Let us retrain this architecture on the same and observe the difference.
